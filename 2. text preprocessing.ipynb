{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shaonsikder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shaonsikder/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shaonsikder/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shaonsikder/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Hello! This is an example sentence. It contains some numbers like 123 and HTML tags <b>like this</b>.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML tags removal:\n",
    "*Strips out HTML markup from text. This is crucial when processing web-scraped data to get clean, readable text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. HTML tags removal:\n",
      "Hello! This is an example sentence. It contains some numbers like 123 and HTML tags like this.\n"
     ]
    }
   ],
   "source": [
    "# 1. HTML tags removal\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "print(\"\\n1. HTML tags removal:\")\n",
    "print(remove_html_tags(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal:\n",
    "*Eliminates punctuation marks from the text. This can help reduce noise in the data and simplify further processing.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Punctuation removal:\n",
      "Hello This is an example sentence It contains some numbers like 123 and HTML tags blike thisb\n"
     ]
    }
   ],
   "source": [
    "# 2. Punctuation removal\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "print(\"\\n2. Punctuation removal:\")\n",
    "print(remove_punctuation(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers removal:\n",
    "*Removes numerical digits from the text. This is often done when numbers aren't relevant to the analysis or to reduce vocabulary size.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Numbers removal:\n",
      "Hello! This is an example sentence. It contains some numbers like  and HTML tags <b>like this</b>.\n"
     ]
    }
   ],
   "source": [
    "# 3. Numbers removal\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "print(\"\\n3. Numbers removal:\")\n",
    "print(remove_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming:\n",
    "*Reduces words to their root or base form. For example, \"running\" becomes \"run\". It's a crude heuristic process that chops off word endings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Stemming:\n",
      "['hello', '!', 'thi', 'is', 'an', 'exampl', 'sentenc', '.', 'it', 'contain', 'some', 'number', 'like', '123', 'and', 'html', 'tag', '<', 'b', '>', 'like', 'thi', '<', '/b', '>', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Stemming\n",
    "def stem_words(text):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return [ps.stem(word) for word in words]\n",
    "\n",
    "print(\"\\n4. Stemming:\") # Definition of stemming: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "print(stem_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization:\n",
    "*Similar to stemming, but aims to return the base or dictionary form of a word. It's more sophisticated than stemming and uses linguistic knowledge to achieve a proper root word.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Lemmatization:\n",
      "['Hello', '!', 'This', 'is', 'an', 'example', 'sentence', '.', 'It', 'contains', 'some', 'number', 'like', '123', 'and', 'HTML', 'tag', '<', 'b', '>', 'like', 'this', '<', '/b', '>', '.']\n"
     ]
    }
   ],
   "source": [
    "# 5. Lemmatization\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(\"\\n5. Lemmatization:\") # Definition of lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "print(lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging:\n",
    "*Assigns part-of-speech tags (like noun, verb, adjective) to each word in the text. This provides grammatical information that can be useful for further analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. POS tagging:\n",
      "[('Hello', 'NN'), ('!', '.'), ('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.'), ('It', 'PRP'), ('contains', 'VBZ'), ('some', 'DT'), ('numbers', 'NNS'), ('like', 'IN'), ('123', 'CD'), ('and', 'CC'), ('HTML', 'NNP'), ('tags', 'VBP'), ('<', 'NNP'), ('b', 'NN'), ('>', 'NN'), ('like', 'IN'), ('this', 'DT'), ('<', 'NNP'), ('/b', 'NNP'), ('>', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 6. POS tagging\n",
    "def pos_tag_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return pos_tag(words)\n",
    "\n",
    "print(\"\\n6. POS tagging:\")\n",
    "print(pos_tag_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal:\n",
    "*Filters out common words that often don't carry much meaning, such as \"the\", \"is\", \"at\". This helps focus on the more important words in the text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Stopwords removal\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "print(\"\\n7. Stopwords removal:\")\n",
    "print(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "*Breaks down text into individual words or subwords. It's the process of splitting a string of text into smaller units called tokens.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Tokenization:\n",
      "['Hello', '!', 'This', 'is', 'an', 'example', 'sentence', '.', 'It', 'contains', 'some', 'numbers', 'like', '123', 'and', 'HTML', 'tags', '<', 'b', '>', 'like', 'this', '<', '/b', '>', '.']\n"
     ]
    }
   ],
   "source": [
    "# 8. Tokenization\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "print(\"8. Tokenization:\")\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
